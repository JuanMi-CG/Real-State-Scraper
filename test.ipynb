{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import inmocasal\n",
    "import axius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 16:55:27,029 - srap_logging - INFO - Skipping download: axius\\page_1.html already exists.\n",
      "2024-12-07 16:55:31,215 - srap_logging - INFO - Total Properties: 147\n",
      "2024-12-07 16:55:31,216 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:31,216 - srap_logging - INFO - Scraping page 1/13...\n",
      "2024-12-07 16:55:31,217 - srap_logging - INFO - Skipping download: axius\\page_1.html already exists.\n",
      "2024-12-07 16:55:31,276 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:31,277 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:31,278 - srap_logging - INFO - Scraping page 2/13...\n",
      "2024-12-07 16:55:31,279 - srap_logging - INFO - Skipping download: axius\\page_2.html already exists.\n",
      "2024-12-07 16:55:31,345 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:31,346 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:31,347 - srap_logging - INFO - Scraping page 3/13...\n",
      "2024-12-07 16:55:31,348 - srap_logging - INFO - Skipping download: axius\\page_3.html already exists.\n",
      "2024-12-07 16:55:31,413 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:31,414 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:31,414 - srap_logging - INFO - Scraping page 4/13...\n",
      "2024-12-07 16:55:31,415 - srap_logging - INFO - Skipping download: axius\\page_4.html already exists.\n",
      "2024-12-07 16:55:31,484 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:31,485 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:31,486 - srap_logging - INFO - Scraping page 5/13...\n",
      "2024-12-07 16:55:31,487 - srap_logging - INFO - Skipping download: axius\\page_5.html already exists.\n",
      "2024-12-07 16:55:31,561 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:31,562 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:31,562 - srap_logging - INFO - Scraping page 6/13...\n",
      "2024-12-07 16:55:31,563 - srap_logging - INFO - Skipping download: axius\\page_6.html already exists.\n",
      "2024-12-07 16:55:31,643 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:31,644 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:31,645 - srap_logging - INFO - Scraping page 7/13...\n",
      "2024-12-07 16:55:31,645 - srap_logging - INFO - Skipping download: axius\\page_7.html already exists.\n",
      "2024-12-07 16:55:31,718 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:31,719 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:31,720 - srap_logging - INFO - Scraping page 8/13...\n",
      "2024-12-07 16:55:31,721 - srap_logging - INFO - Skipping download: axius\\page_8.html already exists.\n",
      "2024-12-07 16:55:31,799 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:31,800 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:31,801 - srap_logging - INFO - Scraping page 9/13...\n",
      "2024-12-07 16:55:31,802 - srap_logging - INFO - Skipping download: axius\\page_9.html already exists.\n",
      "2024-12-07 16:55:31,970 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:31,971 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:31,972 - srap_logging - INFO - Scraping page 10/13...\n",
      "2024-12-07 16:55:31,973 - srap_logging - INFO - Skipping download: axius\\page_10.html already exists.\n",
      "2024-12-07 16:55:32,051 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:32,052 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:32,052 - srap_logging - INFO - Scraping page 11/13...\n",
      "2024-12-07 16:55:32,054 - srap_logging - INFO - Skipping download: axius\\page_11.html already exists.\n",
      "2024-12-07 16:55:32,128 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:32,129 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:32,130 - srap_logging - INFO - Scraping page 12/13...\n",
      "2024-12-07 16:55:32,131 - srap_logging - INFO - Skipping download: axius\\page_12.html already exists.\n",
      "2024-12-07 16:55:32,206 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:32,207 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:32,209 - srap_logging - INFO - Scraping page 13/13...\n",
      "2024-12-07 16:55:32,210 - srap_logging - INFO - Skipping download: axius\\page_13.html already exists.\n",
      "2024-12-07 16:55:32,256 - srap_logging - INFO - Number of items: 227\n"
     ]
    }
   ],
   "source": [
    "axius.scrap('axius')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 16:55:37,866 - srap_logging - INFO - Skipping download: inmocasal\\page_1.html already exists.\n",
      "2024-12-07 16:55:37,948 - srap_logging - INFO - Número de páginas: 9\n",
      "2024-12-07 16:55:37,949 - srap_logging - INFO - Total Properties: 9\n",
      "2024-12-07 16:55:37,950 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:37,952 - srap_logging - INFO - Scraping page 1/9...\n",
      "2024-12-07 16:55:37,953 - srap_logging - INFO - Skipping download: inmocasal\\page_1.html already exists.\n",
      "2024-12-07 16:55:38,049 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:38,050 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:38,050 - srap_logging - INFO - Scraping page 2/9...\n",
      "2024-12-07 16:55:38,051 - srap_logging - INFO - Skipping download: inmocasal\\page_2.html already exists.\n",
      "2024-12-07 16:55:38,140 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:38,142 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:38,144 - srap_logging - INFO - Scraping page 3/9...\n",
      "2024-12-07 16:55:38,144 - srap_logging - INFO - Skipping download: inmocasal\\page_3.html already exists.\n",
      "2024-12-07 16:55:38,230 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:38,232 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:38,233 - srap_logging - INFO - Scraping page 4/9...\n",
      "2024-12-07 16:55:38,234 - srap_logging - INFO - Skipping download: inmocasal\\page_4.html already exists.\n",
      "2024-12-07 16:55:42,478 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:42,479 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:42,480 - srap_logging - INFO - Scraping page 5/9...\n",
      "2024-12-07 16:55:42,481 - srap_logging - INFO - Skipping download: inmocasal\\page_5.html already exists.\n",
      "2024-12-07 16:55:42,559 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:42,560 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:42,561 - srap_logging - INFO - Scraping page 6/9...\n",
      "2024-12-07 16:55:42,561 - srap_logging - INFO - Skipping download: inmocasal\\page_6.html already exists.\n",
      "2024-12-07 16:55:42,636 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:42,637 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:42,637 - srap_logging - INFO - Scraping page 7/9...\n",
      "2024-12-07 16:55:42,638 - srap_logging - INFO - Skipping download: inmocasal\\page_7.html already exists.\n",
      "2024-12-07 16:55:42,714 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:42,715 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:42,716 - srap_logging - INFO - Scraping page 8/9...\n",
      "2024-12-07 16:55:42,717 - srap_logging - INFO - Skipping download: inmocasal\\page_8.html already exists.\n",
      "2024-12-07 16:55:42,792 - srap_logging - INFO - Number of items: 227\n",
      "2024-12-07 16:55:42,793 - srap_logging - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2024-12-07 16:55:42,794 - srap_logging - INFO - Scraping page 9/9...\n",
      "2024-12-07 16:55:42,795 - srap_logging - INFO - Skipping download: inmocasal\\page_9.html already exists.\n",
      "2024-12-07 16:55:42,868 - srap_logging - INFO - Number of items: 227\n"
     ]
    }
   ],
   "source": [
    "inmocasal.scrap('inmocasal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/new_properties.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cargar los DataFrames\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/properties.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df_new \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/new_properties.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Imprimir la longitud de cada DataFrame\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitud properties: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\PROYECTOS\\real_state_scraper\\venv\\Lib\\site-packages\\pandas\\io\\pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\PROYECTOS\\real_state_scraper\\venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/new_properties.pkl'"
     ]
    }
   ],
   "source": [
    "# Cargar los DataFrames\n",
    "df = pd.read_pickle('results/properties.pkl')\n",
    "df_new = pd.read_pickle('results/new_properties.pkl')\n",
    "\n",
    "# Imprimir la longitud de cada DataFrame\n",
    "print(f'Longitud properties: {len(df)}')\n",
    "print(f'Longitud new properties: {len(df_new)}')\n",
    "\n",
    "# Contar elementos por categorías en la columna 'inmobiliaria'\n",
    "categorias_df = df['inmobiliaria'].value_counts()\n",
    "categorias_df_new = df_new['inmobiliaria'].value_counts()\n",
    "\n",
    "# Imprimir conteo por categorías\n",
    "print(\"\\nConteo de categorías en 'properties':\")\n",
    "print(categorias_df)\n",
    "\n",
    "print(\"\\nConteo de categorías en 'new_properties':\")\n",
    "print(categorias_df_new)\n",
    "\n",
    "# Imprimir totales\n",
    "total_properties = categorias_df.sum()\n",
    "total_new_properties = categorias_df_new.sum()\n",
    "\n",
    "print(f\"\\nTotal de elementos en 'properties' por categorías: {total_properties}\")\n",
    "print(f\"Total de elementos en 'new_properties' por categorías: {total_new_properties}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELIMINAR REGISTROS PARA PRUEBA DE NUEVAS VIVIENDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['inmobiliaria'] != 'inmocasal']\n",
    "df.to_pickle('results/properties.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad inicial de registros:\n",
      "227\n",
      "Cantidad final de registros:\n",
      "221\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el DataFrame desde el archivo pickle\n",
    "df = pd.read_pickle('results/properties.pkl')\n",
    "print(\"Cantidad inicial de registros:\", len(df))\n",
    "\n",
    "# Agrupar por la columna de inmobiliarias y eliminar 3 registros de cada grupo\n",
    "for name, group in df.groupby('inmobiliaria'):\n",
    "    if len(group) > 3:  # Verificar que haya al menos 3 registros\n",
    "        # Seleccionar los 3 primeros registros del grupo para eliminar\n",
    "        to_delete = group.iloc[:3].index\n",
    "        \n",
    "        # Eliminar los registros del DataFrame original\n",
    "        df = df.drop(to_delete)\n",
    "\n",
    "# Guardar el DataFrame actualizado en un archivo pickle\n",
    "df.to_pickle('results/properties.pkl')\n",
    "print(\"Cantidad final de registros:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud properties: 221\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('results/properties.pkl')\n",
    "print(f'Longitud properties: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
